{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartoonGAN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuigiSigillo/CartoonGAN/blob/main/CartoonGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMbzNjd_vMhL"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXZ-VexSrDLg"
      },
      "source": [
        "from google.colab import drive\r\n",
        "import os\r\n",
        "import json\r\n",
        "import re\r\n",
        "import cv2\r\n",
        "import numpy as np\r\n",
        "from PIL import Image, ImageFilter\r\n",
        "import sys\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "drive.mount('/content/drive')\r\n",
        "#!unzip /content/drive/My\\ Drive/NN/spirited_away.zip -d /content/drive/My\\ Drive/NN/\r\n",
        "!ls /content/drive/My\\ Drive/NN/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baJ4P2PZHEyK"
      },
      "source": [
        "# 1 - Image Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGA5HMGtdieJ"
      },
      "source": [
        "### 1.1 - Resizing images\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHyCYxUUOLPx"
      },
      "source": [
        "def resize(path):\r\n",
        "    for item in os.listdir(path):\r\n",
        "            im = Image.open(path+item)\r\n",
        "            f, e = os.path.splitext(item)\r\n",
        "            imResize = im.resize((256,256), Image.ANTIALIAS)\r\n",
        "            print(f)\r\n",
        "            imResize.save(path+\"_resized/\" + f + ' resized.jpg', 'JPEG', quality=90)\r\n",
        "\r\n",
        "resize('/content/drive/My Drive/NN/your_name')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhGxFlNBd5S1"
      },
      "source": [
        "### 1.2 Apply canny, dilate edge and gaussian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0c8rFFBcs8X"
      },
      "source": [
        "def edge_smoothing(cartoon_images_filename, smoothed_images_filename):\r\n",
        "    print(\"Edge-smoothing of \", cartoon_images_filename)\r\n",
        "    origin = cv2.imread(cartoon_images_filename)\r\n",
        "    edges = createEdgesOverlay(origin)\r\n",
        "    result = overlayEdges(edges, origin)\r\n",
        "    #show_images(origin, edges, result)\r\n",
        "    result.save(smoothed_images_filename, \"JPEG\")\r\n",
        "\r\n",
        "def overlayEdges(edges, origin):\r\n",
        "    background = transformFromCV2ToPillowImageFormat(origin)\r\n",
        "    background.paste(edges, (0, 0), edges)\r\n",
        "    background = background.convert(\"RGB\")\r\n",
        "    return background\r\n",
        "\r\n",
        "def transformFromCV2ToPillowImageFormat(img):\r\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGBA)\r\n",
        "    return Image.fromarray(img)\r\n",
        "\r\n",
        "def createEdgesOverlay(origin):\r\n",
        "    edges = cv2.Canny(origin, 30, 300, 3) \r\n",
        "    edges = cv2.dilate(edges, (3, 3))\r\n",
        "    edges = cv2.bitwise_not(edges)\r\n",
        "    edges = transformFromCV2ToPillowImageFormat(edges)\r\n",
        "    makeWhiteBackgroundTransparent(edges)\r\n",
        "    edges = edges.filter(ImageFilter.GaussianBlur) #do blurring here because doing it before making background transparent results in white halo\r\n",
        "\r\n",
        "    return edges\r\n",
        "\r\n",
        "def makeWhiteBackgroundTransparent(img):\r\n",
        "    datas = img.getdata()\r\n",
        "    newData = []\r\n",
        "    for item in datas:\r\n",
        "        if item[0] == 255 and item[1] == 255 and item[2] == 255:\r\n",
        "            newData.append((255, 255, 255, 0))\r\n",
        "        else:\r\n",
        "            newData.append(item)\r\n",
        "    img.putdata(newData)\r\n",
        "\r\n",
        "def show_images(img,edges,result):\r\n",
        "    plt.subplot(131),plt.imshow(img)\r\n",
        "    plt.title('Original Image'), plt.xticks([]), plt.yticks([])\r\n",
        "\r\n",
        "    plt.subplot(132),plt.imshow(edges)\r\n",
        "    plt.title('Edge Image'), plt.xticks([]), plt.yticks([])\r\n",
        "    \r\n",
        "    plt.subplot(133),plt.imshow(result)\r\n",
        "    plt.title('Result Image'), plt.xticks([]), plt.yticks([])\r\n",
        "\r\n",
        "    plt.show()\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "path_resized = \"/content/drive/My Drive/NN/spirited_away_resized/\"\r\n",
        "path_smoothed = \"/content/drive/My Drive/NN/spirited_away_resized_smoothed/\"\r\n",
        "\r\n",
        "\r\n",
        "for filename in os.listdir(path_resized):\r\n",
        "  #filename='scene43626 resized.jpg'\r\n",
        "  f = filename.split(\" \")[0] + \" smoothed\"\r\n",
        "  cartoon_images_filename = path_resized + filename\r\n",
        "  smoothed_images_filename = path_smoothed + f\r\n",
        "  edge_smoothing(cartoon_images_filename, smoothed_images_filename)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5co-q5KoHCGt"
      },
      "source": [
        "# GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDNGVgxFP8b0"
      },
      "source": [
        "## Load VGG19 (transfer learning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS1a5Ra-HD6k"
      },
      "source": [
        "from keras import applications\r\n",
        "from keras.models import Model, Input\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, UpSampling2D\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras import regularizers\r\n",
        "from keras import optimizers\r\n",
        "\r\n",
        "#TODO\r\n",
        "def my_loss_fn(y_true, y_pred):\r\n",
        "    squared_difference = tf.square(y_true - y_pred)\r\n",
        "    return tf.reduce_mean(squared_difference, axis=-1)\r\n",
        "\r\n",
        "\r\n",
        "def load_backbone_net(input_shape):\r\n",
        "    \r\n",
        "    # define input tensor\r\n",
        "    input0 = Input(shape=input_shape)\r\n",
        "\r\n",
        "    # load a pretrained model on imagenet without the final dense layer\r\n",
        "    feature_extractor = applications.vgg19.VGG19(include_top=False, weights='imagenet', input_tensor=input0)\r\n",
        "    \r\n",
        "    feature_extractor = feature_extractor.output\r\n",
        "    feature_extractor = Model(inputs=input0, outputs=feature_extractor)\r\n",
        "    optimizer = 'adam' #alternative 'SGD'\r\n",
        "\r\n",
        "    feature_extractor.compile(loss=my_loss_fn, optimizer=optimizer, metrics=['accuracy'])\r\n",
        "\r\n",
        "    return feature_extractor\r\n",
        "\r\n",
        "\r\n",
        "def transferNet(feature_extractor, output_layer_name, trainable_layers):\r\n",
        "    \r\n",
        "    # get the original input layer tensor\r\n",
        "    input_t = feature_extractor.get_layer(index=0).input\r\n",
        "\r\n",
        "    # set the feture extractor layers as non-trainable\r\n",
        "    for idx,layer in enumerate(feature_extractor.layers):\r\n",
        "      if layer.name in trainable_layers:\r\n",
        "        layer.trainable = True\r\n",
        "      else:\r\n",
        "        layer.trainable = False\r\n",
        "\r\n",
        "    # get the output tensor from a layer of the feature extractor\r\n",
        "    output_extractor = feature_extractor.get_layer(name = output_layer_name).output\r\n",
        "    \r\n",
        "    #output_extractor = MaxPooling2D(pool_size=(4,4))(output_extractor)\r\n",
        "\r\n",
        "    # flat the output of a Conv layer\r\n",
        "    flatten = Flatten()(output_extractor) \r\n",
        "    flatten_norm = BatchNormalization()(flatten)\r\n",
        "\r\n",
        "    # add a Dense layer\r\n",
        "    dense = Dropout(0.4)(flatten_norm)\r\n",
        "    dense = Dense(200, activation='relu')(dense)\r\n",
        "    dense = BatchNormalization()(dense)\r\n",
        "    \r\n",
        "    # add a Dense layer\r\n",
        "    dense = Dropout(0.4)(dense)\r\n",
        "    dense = Dense(100, activation='relu')(dense)\r\n",
        "    dense = BatchNormalization()(dense)    \r\n",
        "\r\n",
        "    model = Model(inputs=input_t, outputs=dense, name=\"transferNet\")\r\n",
        "    \r\n",
        "    optimizer = 'adam' #alternative 'SGD'\r\n",
        "    model.compile(loss=my_loss_fn, optimizer=optimizer, metrics=['accuracy'])\r\n",
        "\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "input_shape =(256,256,3)\r\n",
        "# load the pre-trained model\r\n",
        "feature_extractor = load_backbone_net(input_shape)\r\n",
        "feature_extractor.summary()\r\n",
        "\r\n",
        "\r\n",
        "# choose the layer from which you can get the features \r\n",
        "name_output_extractor = \"block4_conv4\" #ultimo\r\n",
        "trainable_layers = [\"block4_conv3\",\"block4_conv2\"] #penultimo\r\n",
        "\r\n",
        "# build the transfer model\r\n",
        "transfer_model = transferNet(feature_extractor, name_output_extractor, trainable_layers)\r\n",
        "transfer_model.summary()\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCcHgsTRbthT"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "srtyDDuObvfV",
        "outputId": "63fc8f1b-71ed-4ebe-bcb2-a762d3490431"
      },
      "source": [
        "from keras import applications\r\n",
        "from keras import layers\r\n",
        "from keras.models import Model, Input\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import *\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras import regularizers\r\n",
        "from keras import optimizers\r\n",
        "\r\n",
        "\r\n",
        "def residual_block(y, nb_channels):\r\n",
        "    shortcut = y\r\n",
        "    print(\"c\")\r\n",
        "    # down-sampling is performed with a stride of 1\r\n",
        "    y = Conv2D(nb_channels, kernel_size=3, strides=1, padding='valid',activation=\"relu\")(y)\r\n",
        "    y = ZeroPadding2D(padding=(1, 1))(y)\r\n",
        "    y = Conv2D(nb_channels, kernel_size=3, strides=1, padding='valid',activation=\"relu\")(y)\r\n",
        "    y = ZeroPadding2D(padding=(1, 1))(y)\r\n",
        "    y = BatchNormalization()(y)\r\n",
        "    y = BatchNormalization()(y)\r\n",
        "\r\n",
        "    y = layers.add([shortcut, y])\r\n",
        "    return y\r\n",
        "\r\n",
        "\r\n",
        "def generator():\r\n",
        "    model = Sequential()\r\n",
        "\r\n",
        "    model.add(Conv2D(filters=64, kernel_size=7, strides=1, padding='valid',activation=\"relu\"))\r\n",
        "    model.add(ZeroPadding2D(padding=(3, 3)))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    \r\n",
        "    #### Down-Convolution\r\n",
        "    \r\n",
        "    #k3 n128 s2\r\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, strides=2, padding='valid',activation=\"relu\"))\r\n",
        "    model.add(ZeroPadding2D(padding=(1, 1)))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    #k3 n128 s1\r\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, strides=1, padding='valid',activation=\"relu\"))\r\n",
        "    model.add(ZeroPadding2D(padding=(1, 1)))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "\r\n",
        "    #k3 n256 s2\r\n",
        "    model.add(Conv2D(filters=256, kernel_size=3, strides=2, padding='valid',activation=\"relu\"))\r\n",
        "    model.add(ZeroPadding2D(padding=(1, 1)))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    #k3 n256 s1\r\n",
        "    model.add(Conv2D(filters=256, kernel_size=3, strides=1, padding='valid',activation=\"relu\", name=\"test\"))\r\n",
        "    model.add(ZeroPadding2D(padding=(1, 1)))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "\r\n",
        "    \r\n",
        "    # residual blocks\r\n",
        "    for i in range(8):#number of res blocks\r\n",
        "        model.add(residual_block(,256))\r\n",
        "    \r\n",
        "    \r\n",
        "    # Up-convolution\r\n",
        "    model.add(Conv2DTranspose(filters=128, kernel_size=3, strides=2, padding='valid',output_padding=1,activation='relu'))\r\n",
        "    model.add(ZeroPadding2D(padding=(1, 1)))\r\n",
        "    \r\n",
        "    model.add(Conv2DTranspose(filters=128, kernel_size=3, strides=1, padding='valid',activation='relu'))\r\n",
        "    model.add(ZeroPadding2D(padding=(1, 1)))\r\n",
        "    \r\n",
        "    model.add(keras.layers.BatchNormalization())\r\n",
        "    #######################\r\n",
        "    model.add(Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='valid',output_padding=1,activation='relu'))\r\n",
        "    model.add(ZeroPadding2D(padding=(1, 1)))\r\n",
        "    \r\n",
        "    model.add(Conv2DTranspose(filters=64, kernel_size=3, strides=1, padding='valid',activation='relu'))\r\n",
        "    model.add(ZeroPadding2D(padding=(1, 1)))\r\n",
        "\r\n",
        "    model.add(BatchNormalization())\r\n",
        "\r\n",
        "    ###\r\n",
        "    model.add(Conv2D(filters=3, kernel_size=7, strides=1, padding='valid',activation=\"sigmoid\"))\r\n",
        "    model.add(ZeroPadding2D(padding=(3, 3)))\r\n",
        "\r\n",
        "    return model\r\n",
        "\r\n",
        "model = generator()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-6d06829f8b1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-52-6d06829f8b1f>\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# residual blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#number of res blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-6d06829f8b1f>\u001b[0m in \u001b[0;36mresidual_block\u001b[0;34m(y, nb_channels)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# down-sampling is performed with a stride of 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZeroPadding2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m       \u001b[0minput_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0meager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0mcall_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# have a `shape` attribute.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Inputs to a layer should be tensors. Got: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Inputs to a layer should be tensors. Got: <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f4013e1fa90>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdnuXj0RPVcA"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krWmM57TPCiQ"
      },
      "source": [
        "###Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzLDODzQO834"
      },
      "source": [
        "models_dir = \"/content/drive/My Drive/NN/\"\r\n",
        "\r\n",
        "def savemodel(model,problem):\r\n",
        "    filename = os.path.join(models_dir, '%s.h5' %problem)\r\n",
        "    model.save(filename)\r\n",
        "    print(\"\\nModel saved successfully on file %s\\n\" %filename)\r\n",
        "\r\n",
        "# Save the model MWI-Dataset-1.1_models\r\n",
        "savemodel(model,'Twd_model_31_epochs_2000_images')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIfpasF6PGW7"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayYkEJH4PBTr"
      },
      "source": [
        "from keras.models import load_model\r\n",
        "\r\n",
        "models_dir = \"/content/drive/My Drive/NN/\"\r\n",
        "model_name = \"towards_model_100_epochs_2000_images_acc44_256\"\r\n",
        "\r\n",
        "def loadmodel(problem):\r\n",
        "    filename = os.path.join(models_dir, '%s.h5' %problem)\r\n",
        "    try:\r\n",
        "        model = load_model(filename)\r\n",
        "        print(\"\\nModel loaded successfully from file %s\\n\" %filename)\r\n",
        "    except OSError:    \r\n",
        "        print(\"\\nModel file %s not found!!!\\n\" %filename)\r\n",
        "        model = None\r\n",
        "    return model\r\n",
        "\r\n",
        "model = loadmodel(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}