{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartoonGAN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ad4d3c2bdb849298fb962c0d5425752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dfaccf5601964c08b80e986359b4a11e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_aebeb31e5013477c9fd6e0a31110bdac",
              "IPY_MODEL_a162ffbdaea74180833ef06da356b883"
            ]
          }
        },
        "dfaccf5601964c08b80e986359b4a11e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aebeb31e5013477c9fd6e0a31110bdac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b08b66dddd114251962d6097efe2f8db",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3591,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3591,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3db746da824b4f0e9a3111aba702771b"
          }
        },
        "a162ffbdaea74180833ef06da356b883": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d67385da79fc4e3ca2c39aa05c0b999d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3591/3591 [49:14&lt;00:00,  1.22it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b8ca9a9515564f9eb61d5145a4e884d0"
          }
        },
        "b08b66dddd114251962d6097efe2f8db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3db746da824b4f0e9a3111aba702771b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d67385da79fc4e3ca2c39aa05c0b999d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b8ca9a9515564f9eb61d5145a4e884d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuigiSigillo/CartoonGAN/blob/main/CartoonGAN_style.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMbzNjd_vMhL"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXZ-VexSrDLg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5d6f3e0-8f0b-4a0d-fcca-60762a5c3f55"
      },
      "source": [
        "from google.colab import drive\r\n",
        "import os\r\n",
        "import json\r\n",
        "import re\r\n",
        "import cv2\r\n",
        "import numpy as np\r\n",
        "from PIL import Image, ImageFilter\r\n",
        "import sys\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "import tensorflow as tf\r\n",
        "!pip install tensorflow_addons\r\n",
        "import tensorflow_addons as tfa\r\n",
        "from tensorflow.python.client import device_lib\r\n",
        "import keras\r\n",
        "#!unzip /content/drive/My\\ Drive/NN/spirited_away.zip -d /content/drive/My\\ Drive/NN/\r\n",
        "#!ls /content/drive/My\\ Drive/NN/\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "if device_name != '/device:GPU:0':\r\n",
        "  print('GPU device not found')\r\n",
        "else:\r\n",
        "    print('Found GPU at: {}'.format(device_name))\r\n",
        "    print(device_lib.list_local_devices()[1])\r\n",
        "drive.mount('/content/drive')\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/e3/56d2fe76f0bb7c88ed9b2a6a557e25e83e252aec08f13de34369cd850a0b/tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703kB)\n",
            "\r\u001b[K     |▌                               | 10kB 23.4MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 14.7MB/s eta 0:00:01\r\u001b[K     |█▍                              | 30kB 13.2MB/s eta 0:00:01\r\u001b[K     |█▉                              | 40kB 12.3MB/s eta 0:00:01\r\u001b[K     |██▎                             | 51kB 7.6MB/s eta 0:00:01\r\u001b[K     |██▉                             | 61kB 8.3MB/s eta 0:00:01\r\u001b[K     |███▎                            | 71kB 8.3MB/s eta 0:00:01\r\u001b[K     |███▊                            | 81kB 9.3MB/s eta 0:00:01\r\u001b[K     |████▏                           | 92kB 8.9MB/s eta 0:00:01\r\u001b[K     |████▋                           | 102kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 112kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 122kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 133kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 143kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 153kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 163kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 174kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 184kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 194kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 204kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 215kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 225kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 235kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 245kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 256kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 266kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 276kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 286kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 296kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 307kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 317kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 327kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 337kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 348kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 358kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 368kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 378kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 389kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 399kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 409kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 419kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 430kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 440kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 450kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 460kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 471kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 481kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 491kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 501kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 512kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 522kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 532kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 542kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 552kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 563kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 573kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 583kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 593kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 604kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 614kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 624kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 634kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 645kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 655kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 665kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 675kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 686kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 696kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 706kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.12.1\n",
            "Found GPU at: /device:GPU:0\n",
            "name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14674281152\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 9182306827457485587\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdnuXj0RPVcA"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krWmM57TPCiQ"
      },
      "source": [
        "###Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzLDODzQO834"
      },
      "source": [
        "models_dir = \"/content/drive/My Drive/NN/models\"\r\n",
        "checkpoint_dir = \"/content/drive/My Drive/NN/models/checkpoint\"\r\n",
        "\r\n",
        "def savemodel(model,problem,checkpoint=False):\r\n",
        "    if checkpoint:\r\n",
        "        filename = os.path.join(checkpoint_dir, DATASET_NAME, '%s.h5' %problem)\r\n",
        "        '''try:\r\n",
        "            act_epoch_check = int(problem[len(a)-5:len(problem)-2]) #3 digits\r\n",
        "        except:\r\n",
        "            act_epoch_check = int(problem[len(problem)-4:len(problem)-2]) #2 digits\r\n",
        "        prec_epoch_check = act_epoch_check - CHECK_AFTER_X_EPOCH\r\n",
        "        old_check_g = MODEL_NAME_GEN+\"_checkpoint_\"+str(prec_epoch_check)+\"ep\"\r\n",
        "        old_check_d = MODEL_NAME_DISC+\"_checkpoint_\"+str(prec_epoch_check)+\"ep\"\r\n",
        "        !rm \"/content/drive/My Drive/NN/models/checkpoint/{DATASET_NAME}/{old_check_g}.h5\" \r\n",
        "        !rm \"/content/drive/My Drive/NN/models/checkpoint/{DATASET_NAME}/{old_check_d}.h5\" '''\r\n",
        "        \r\n",
        "    else:\r\n",
        "        filename = os.path.join(models_dir,DATASET_NAME, '%s.h5' %problem)\r\n",
        "    model.save(filename)\r\n",
        "    print(\"\\nModel saved successfully on file %s\\n\" %filename)\r\n",
        "\r\n",
        "#savemodel(D,'discriminator_0')\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIfpasF6PGW7"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayYkEJH4PBTr"
      },
      "source": [
        "from keras.models import load_model\r\n",
        "from keras.layers import *\r\n",
        "models_dir = \"/content/drive/My Drive/NN/models\"\r\n",
        "checkpoint_dir = \"/content/drive/My Drive/NN/models/checkpoint\"\r\n",
        "\r\n",
        "def loadmodel(problem, checkpoint=False):\r\n",
        "    if checkpoint:\r\n",
        "        filename = os.path.join(checkpoint_dir,DATASET_NAME, '%s.h5' %problem)\r\n",
        "    elif \"pretrain\" in problem:\r\n",
        "        filename = os.path.join(models_dir, '%s.h5' %problem)\r\n",
        "    else:\r\n",
        "        filename = os.path.join(models_dir,DATASET_NAME, '%s.h5' %problem)\r\n",
        "    try:\r\n",
        "        model = load_model(filename)\r\n",
        "        print(\"\\nModel loaded successfully from file %s\\n\" %filename)\r\n",
        "    except OSError:    \r\n",
        "        print(\"\\nModel file %s not found!!!\\n\" %filename)\r\n",
        "        model = None\r\n",
        "    return model\r\n",
        "\r\n",
        "#gen_pretrained = loadmodel(model_name)\r\n",
        "#gen_trained = loadmodel(\"generator_01\")\r\n",
        "#disc_trained = loadmodel(\"discriminator_01\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-YKrNy03kop"
      },
      "source": [
        "### Load datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP71uC9H3SLE"
      },
      "source": [
        "'''#load imgs ?\r\n",
        "data_dir = '/content/drive/My Drive/NN/photos_rszd'\r\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,   validation_split=0.2, horizontal_flip=True)\r\n",
        "\r\n",
        "train_gen = train_datagen.flow_from_directory(\r\n",
        "        data_dir,\r\n",
        "        subset=\"training\",\r\n",
        "        seed=123,\r\n",
        "        target_size=(256, 256),\r\n",
        "        batch_size=32\r\n",
        "        )\r\n",
        "validation_generator = train_datagen.flow_from_directory(\r\n",
        "        data_dir,\r\n",
        "        target_size=(256,256),\r\n",
        "        batch_size=32,\r\n",
        "        )\r\n",
        "'''\r\n",
        "from glob import glob\r\n",
        "def get_dataset(dataset_name, batch_size):\r\n",
        "    files = glob(os.path.join('/content/drive/My Drive/NN/', dataset_name, \"*\"))\r\n",
        "    num_images = len(files)\r\n",
        "    print(f\"Found {num_images}  images in {dataset_name} folder.\")\r\n",
        "    ds = tf.data.Dataset.from_tensor_slices(files)\r\n",
        "    ds = ds.shuffle(num_images)\r\n",
        "    ds = ds.repeat()\r\n",
        "\r\n",
        "    def fn(filename):\r\n",
        "        x = tf.io.read_file(filename)\r\n",
        "        x = tf.image.decode_jpeg(x, channels=3)\r\n",
        "        img = tf.cast(x, tf.float32) / 127.5 - 1\r\n",
        "        #print(\"\\n tipo img = \",type(img),\"\\n tipo x = \",type(x), filename, type(filename))\r\n",
        "        return img\r\n",
        "\r\n",
        "    ds = ds.map(fn, batch_size)\r\n",
        "    ds = ds.batch(batch_size)\r\n",
        "    \r\n",
        "    steps = int(np.ceil(num_images/batch_size))\r\n",
        "    # user iter(ds) to avoid generating iterator every epoch\r\n",
        "    return iter(ds), steps\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baJ4P2PZHEyK"
      },
      "source": [
        "# 1 - Image Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGA5HMGtdieJ"
      },
      "source": [
        "### 1.1 - Resizing images\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHyCYxUUOLPx"
      },
      "source": [
        "def resize(path):\r\n",
        "    for item in os.listdir(path):\r\n",
        "            im = Image.open(os.path.join(path,item))\r\n",
        "            f, e = os.path.splitext(item)\r\n",
        "            imResize = im.resize((256,256), Image.ANTIALIAS)\r\n",
        "            print(f)\r\n",
        "            imResize.save(path+\"_resized/\" + f + ' resized.jpg', 'JPEG', quality=90)\r\n",
        "\r\n",
        "#!unzip /content/drive/My\\ Drive/NN/photos_from_COCO.zip -d /content/drive/My\\ Drive/NN/\r\n",
        "resize('/content/drive/My Drive/NN/datasets/test_img/test_img_resized')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhGxFlNBd5S1"
      },
      "source": [
        "### 1.2a Apply canny, dilate edge and gaussian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0c8rFFBcs8X"
      },
      "source": [
        "def edge_smoothing(cartoon_images_filename, smoothed_images_filename):\r\n",
        "    print(\"Edge-smoothing of \", cartoon_images_filename)\r\n",
        "    origin = cv2.imread(cartoon_images_filename)\r\n",
        "    edges = createEdgesOverlay(origin)\r\n",
        "    result = overlayEdges(edges, origin)\r\n",
        "    #show_images(origin, edges, result)\r\n",
        "    result.save(smoothed_images_filename, \"JPEG\")\r\n",
        "\r\n",
        "def overlayEdges(edges, origin):\r\n",
        "    background = transformFromCV2ToPillowImageFormat(origin)\r\n",
        "    background.paste(edges, (0, 0), edges)\r\n",
        "    background = background.convert(\"RGB\")\r\n",
        "    return background\r\n",
        "\r\n",
        "def transformFromCV2ToPillowImageFormat(img):\r\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGBA)\r\n",
        "    return Image.fromarray(img)\r\n",
        "\r\n",
        "def createEdgesOverlay(origin):\r\n",
        "    edges = cv2.Canny(origin, 30, 300, 3) \r\n",
        "    edges = cv2.dilate(edges, (3, 3))\r\n",
        "    edges = cv2.bitwise_not(edges)\r\n",
        "    edges = transformFromCV2ToPillowImageFormat(edges)\r\n",
        "    makeWhiteBackgroundTransparent(edges)\r\n",
        "    edges = edges.filter(ImageFilter.GaussianBlur) #do blurring here because doing it before making background transparent results in white halo\r\n",
        "\r\n",
        "    return edges\r\n",
        "\r\n",
        "def makeWhiteBackgroundTransparent(img):\r\n",
        "    datas = img.getdata()\r\n",
        "    newData = []\r\n",
        "    for item in datas:\r\n",
        "        if item[0] == 255 and item[1] == 255 and item[2] == 255:\r\n",
        "            newData.append((255, 255, 255, 0))\r\n",
        "        else:\r\n",
        "            newData.append(item)\r\n",
        "    img.putdata(newData)\r\n",
        "\r\n",
        "def show_images(img,edges,result):\r\n",
        "    plt.subplot(131),plt.imshow(img)\r\n",
        "    plt.title('Original Image'), plt.xticks([]), plt.yticks([])\r\n",
        "\r\n",
        "    plt.subplot(132),plt.imshow(edges)\r\n",
        "    plt.title('Edge Image'), plt.xticks([]), plt.yticks([])\r\n",
        "    \r\n",
        "    plt.subplot(133),plt.imshow(result)\r\n",
        "    plt.title('Result Image'), plt.xticks([]), plt.yticks([])\r\n",
        "\r\n",
        "    plt.show()\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "path_resized = \"/content/drive/My Drive/NN/spirited_away_resized/\"\r\n",
        "path_smoothed = \"/content/drive/My Drive/NN/spirited_away_resized_smoothed/\"\r\n",
        "\r\n",
        "\r\n",
        "for filename in os.listdir(path_resized):\r\n",
        "  #filename='scene43626 resized.jpg'\r\n",
        "  f = filename.split(\" \")[0] + \" smoothed\"\r\n",
        "  cartoon_images_filename = path_resized + filename\r\n",
        "  smoothed_images_filename = path_smoothed + f\r\n",
        "  edge_smoothing(cartoon_images_filename, smoothed_images_filename)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrP5ufQwa7X6"
      },
      "source": [
        "### 1.2b Apply canny, dilate edge and gaussian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJF1qQUsa9zl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "0ad4d3c2bdb849298fb962c0d5425752",
            "dfaccf5601964c08b80e986359b4a11e",
            "aebeb31e5013477c9fd6e0a31110bdac",
            "a162ffbdaea74180833ef06da356b883",
            "b08b66dddd114251962d6097efe2f8db",
            "3db746da824b4f0e9a3111aba702771b",
            "d67385da79fc4e3ca2c39aa05c0b999d",
            "b8ca9a9515564f9eb61d5145a4e884d0"
          ]
        },
        "outputId": "600f9a36-f183-4d1b-83a8-e7db6bf0d7ee"
      },
      "source": [
        "import numpy as np\r\n",
        "import cv2, os, argparse\r\n",
        "from glob import glob\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "\r\n",
        "def make_edge_smooth(dataset_name, img_size) :\r\n",
        "    file_list = glob('/content/drive/MyDrive/NN/datasets/{}/{}_resized/*.*'.format(dataset_name,dataset_name))\r\n",
        "    save_dir = '/content/drive/MyDrive/NN/datasets/{}/{}_resized_smoothed_2'.format(dataset_name,dataset_name)\r\n",
        "\r\n",
        "    kernel_size = 5\r\n",
        "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\r\n",
        "    gauss = cv2.getGaussianKernel(kernel_size, 0)\r\n",
        "    gauss = gauss * gauss.transpose(1, 0)\r\n",
        "\r\n",
        "    for f in tqdm(file_list) :\r\n",
        "        file_name = os.path.basename(f)\r\n",
        "\r\n",
        "        bgr_img = cv2.imread(f)\r\n",
        "        gray_img = cv2.imread(f, 0)\r\n",
        "\r\n",
        "        bgr_img = cv2.resize(bgr_img, (img_size, img_size))\r\n",
        "        pad_img = np.pad(bgr_img, ((2, 2), (2, 2), (0, 0)), mode='reflect')\r\n",
        "        gray_img = cv2.resize(gray_img, (img_size, img_size))\r\n",
        "\r\n",
        "        edges = cv2.Canny(gray_img, 100, 200)\r\n",
        "        dilation = cv2.dilate(edges, kernel)\r\n",
        "\r\n",
        "        gauss_img = np.copy(bgr_img)\r\n",
        "        idx = np.where(dilation != 0)\r\n",
        "        for i in range(np.sum(dilation != 0)):\r\n",
        "            gauss_img[idx[0][i], idx[1][i], 0] = np.sum(\r\n",
        "                np.multiply(pad_img[idx[0][i]:idx[0][i] + kernel_size, idx[1][i]:idx[1][i] + kernel_size, 0], gauss))\r\n",
        "            gauss_img[idx[0][i], idx[1][i], 1] = np.sum(\r\n",
        "                np.multiply(pad_img[idx[0][i]:idx[0][i] + kernel_size, idx[1][i]:idx[1][i] + kernel_size, 1], gauss))\r\n",
        "            gauss_img[idx[0][i], idx[1][i], 2] = np.sum(\r\n",
        "                np.multiply(pad_img[idx[0][i]:idx[0][i] + kernel_size, idx[1][i]:idx[1][i] + kernel_size, 2], gauss))\r\n",
        "\r\n",
        "        cv2.imwrite(os.path.join(save_dir, file_name.replace(\".jpg\",\"_smoothed.jpg\")), gauss_img)\r\n",
        "make_edge_smooth(\"paprika\",256)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ad4d3c2bdb849298fb962c0d5425752",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=3591.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHaKZTnPsBn6"
      },
      "source": [
        "# 2 - Generator Initialization Phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDNGVgxFP8b0"
      },
      "source": [
        "### Load VGG19 weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS1a5Ra-HD6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b458da8-acdc-4192-b365-5235d41fc376"
      },
      "source": [
        "from keras import applications\r\n",
        "from keras.models import Model, Input\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, UpSampling2D\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras import regularizers\r\n",
        "from keras import optimizers\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def transferNet(input_shape):\r\n",
        "    \r\n",
        "    # download model\r\n",
        "    base_model = applications.vgg19.VGG19(weights=\"imagenet\", include_top=False, input_shape=input_shape)\r\n",
        "    # get the output tensor from a layer of the feature extractor\r\n",
        "    tmp_vgg_output = base_model.get_layer(\"block4_conv3\").output\r\n",
        "    tmp_vgg_output = Conv2D(512, (3, 3), activation='linear', padding='same',name='block4_conv4')(tmp_vgg_output)\r\n",
        "    \r\n",
        "    vgg = Model(inputs=base_model.input, outputs=tmp_vgg_output)\r\n",
        "    vgg.load_weights(os.path.expanduser(os.path.join(\"~\", \".keras\", \"models\",\"vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\")), by_name=True)\r\n",
        "\r\n",
        "    return vgg\r\n",
        "\r\n",
        "\r\n",
        "input_shape =(256,256,3)\r\n",
        "# load the pre-trained model\r\n",
        "vgg = transferNet(input_shape)\r\n",
        "vgg.summary()\r\n",
        "!ls ~/.keras/models\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 0s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv4 (Conv2D)        (None, 64, 64, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv4 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
            "=================================================================\n",
            "Total params: 10,585,152\n",
            "Trainable params: 10,585,152\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5co-q5KoHCGt"
      },
      "source": [
        "#3 - GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCcHgsTRbthT"
      },
      "source": [
        "### Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srtyDDuObvfV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd557df3-05c9-456a-e5d0-d522e3604350"
      },
      "source": [
        "from keras import applications\r\n",
        "from keras import layers\r\n",
        "from keras.models import Model, Input\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import *\r\n",
        "from keras import regularizers\r\n",
        "from keras import optimizers\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_addons as tfa\r\n",
        "\r\n",
        "\r\n",
        "def resblock(x_init):\r\n",
        "    #padding = (3 - 1) // 2\r\n",
        "    #padding = (padding, padding)\r\n",
        "    #x = (ZeroPadding2D(padding=padding))(x_init)\r\n",
        "    x = Conv2D(256, kernel_size=3,padding='same')(x_init)\r\n",
        "\r\n",
        "    x = tfa.layers.InstanceNormalization(axis=3, \r\n",
        "                                   center=True, \r\n",
        "                                   scale=True,\r\n",
        "                                   beta_initializer=\"random_uniform\",\r\n",
        "                                   gamma_initializer=\"random_uniform\",\r\n",
        "                                   epsilon=1e-05)(x)\r\n",
        "    x = tf.nn.relu(x)\r\n",
        "    #x = (ZeroPadding2D(padding=padding))(x)\r\n",
        "    x = (Conv2D(256, kernel_size=3,padding=\"same\"))(x)\r\n",
        "    x = tfa.layers.InstanceNormalization(axis=3, \r\n",
        "                                   center=True, \r\n",
        "                                   scale=True,\r\n",
        "                                   beta_initializer=\"random_uniform\",\r\n",
        "                                   gamma_initializer=\"random_uniform\",\r\n",
        "                                   epsilon=1e-05)(x)    \r\n",
        "\r\n",
        "    return keras.layers.add([x,x_init])\r\n",
        "\r\n",
        "def create_generator(input_shape):\r\n",
        "    #model = Sequential()\r\n",
        "    #k7n64s1\r\n",
        "    input_t = tf.keras.layers.Input(shape=input_shape)\r\n",
        "    \r\n",
        "    '''padding = (7 - 1) // 2\r\n",
        "    padding = (padding, padding)\r\n",
        "    model = (ZeroPadding2D(padding=padding))(input_t)'''\r\n",
        "    \r\n",
        "    model = (Conv2D(filters=64, kernel_size=7,padding=\"same\")(input_t))\r\n",
        "\r\n",
        "    model = tfa.layers.InstanceNormalization(axis=3, \r\n",
        "                                   center=True, \r\n",
        "                                   scale=True,\r\n",
        "                                   beta_initializer=\"random_uniform\",\r\n",
        "                                   gamma_initializer=\"random_uniform\",\r\n",
        "                                   epsilon=1e-05)(model)\r\n",
        "    model = tf.nn.relu(model)\r\n",
        "    \r\n",
        "    #### Down-Convolution\r\n",
        "    \r\n",
        "    #k3 n128 s2\r\n",
        "    '''padding = (3 - 1) // 2\r\n",
        "    padding = (padding, padding)\r\n",
        "    model = (ZeroPadding2D(padding=padding))(model)'''\r\n",
        "    model = (Conv2D(filters=128, kernel_size=3, strides=2,padding=\"same\"))(model)\r\n",
        "    \r\n",
        "    #k3 n128 s1\r\n",
        "    #model = (ZeroPadding2D(padding=padding))(model)\r\n",
        "    model = (Conv2D(filters=128, kernel_size=3,padding=\"same\"))(model)\r\n",
        "    \r\n",
        "    model = tfa.layers.InstanceNormalization(axis=3, \r\n",
        "                                   center=True, \r\n",
        "                                   scale=True,\r\n",
        "                                   beta_initializer=\"random_uniform\",\r\n",
        "                                   gamma_initializer=\"random_uniform\",\r\n",
        "                                   epsilon=1e-05)(model)\r\n",
        "    model = tf.nn.relu(model)\r\n",
        "\r\n",
        "\r\n",
        "    #k3 n256 s2\r\n",
        "    #model = (ZeroPadding2D(padding=padding))(model)\r\n",
        "    model = (Conv2D(filters=256, kernel_size=3, strides=2,padding=\"same\"))(model)\r\n",
        "    \r\n",
        "    #k3 n256 s1\r\n",
        "    #model = (ZeroPadding2D(padding=padding))(model)\r\n",
        "    model = (Conv2D(filters=256, kernel_size=3, padding=\"same\"))(model)\r\n",
        "  \r\n",
        "    model = tfa.layers.InstanceNormalization(axis=3, \r\n",
        "                                   center=True, \r\n",
        "                                   scale=True,\r\n",
        "                                   beta_initializer=\"random_uniform\",\r\n",
        "                                   gamma_initializer=\"random_uniform\",\r\n",
        "                                   epsilon=1e-05)(model)\r\n",
        "    model = tf.nn.relu(model)\r\n",
        "\r\n",
        "\r\n",
        "    # residual blocks\r\n",
        "    for i in range(8):#number of res blocks\r\n",
        "        model = resblock(model)\r\n",
        "\r\n",
        "        \r\n",
        "    # Up-convolution\r\n",
        "    #model = (ZeroPadding2D(padding=padding))(model)\r\n",
        "    model = (Conv2DTranspose(filters=128, kernel_size=3, strides=2,padding=\"same\"))(model)\r\n",
        "    #model = (ZeroPadding2D(padding=padding))(model)\r\n",
        "    model = (Conv2D(filters=128, kernel_size=3,padding=\"same\"))(model)\r\n",
        "    \r\n",
        "    model = tfa.layers.InstanceNormalization(axis=3, \r\n",
        "                                   center=True, \r\n",
        "                                   scale=True,\r\n",
        "                                   beta_initializer=\"random_uniform\",\r\n",
        "                                   gamma_initializer=\"random_uniform\",\r\n",
        "                                   epsilon=1e-05)(model)\r\n",
        "    model = tf.nn.relu(model)\r\n",
        "    #model = tf.keras.backend.resize_images(model, 4, 4, \"channels_last\", 'bilinear')\r\n",
        "    #######################\r\n",
        "\r\n",
        "    #model = (ZeroPadding2D(padding=padding))(model)\r\n",
        "    model = (Conv2DTranspose(filters=64, kernel_size=3, strides=2,padding=\"same\"))(model)\r\n",
        "    #model = (ZeroPadding2D(padding=padding))(model)\r\n",
        "    model = (Conv2D(filters=64, kernel_size=3,padding=\"same\"))(model)\r\n",
        "    \r\n",
        "    model = tfa.layers.InstanceNormalization(axis=3, \r\n",
        "                                   center=True, \r\n",
        "                                   scale=True,\r\n",
        "                                   beta_initializer=\"random_uniform\",\r\n",
        "                                   gamma_initializer=\"random_uniform\",\r\n",
        "                                   epsilon=1e-05)(model)\r\n",
        "    model = tf.nn.relu(model)\r\n",
        "    #model = tf.keras.backend.resize_images(model, 4, 4, \"channels_last\", 'bilinear')\r\n",
        "    ###\r\n",
        "    \r\n",
        "    #padding = (7 - 1) // 2\r\n",
        "    #padding = (padding, padding)\r\n",
        "    #model = (ZeroPadding2D(padding=padding))(model)\r\n",
        "    model = (Conv2D(filters=3, kernel_size=7,padding=\"same\"))(model)\r\n",
        "    \r\n",
        "    model = tf.tanh(model)\r\n",
        "\r\n",
        "    #model.compile(loss=tf.keras.losses.BinaryCrossentropy)\r\n",
        "    model = Model(inputs=input_t, outputs=model, name=\"generator\")\r\n",
        "\r\n",
        "    return model\r\n",
        "\r\n",
        "G = create_generator(input_shape=(256,256,3))\r\n",
        "G.build(input_shape=(256,256,3))\r\n",
        "G.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"generator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_10 (InputLayer)           [(None, 256, 256, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 256, 256, 64) 9472        input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_56 (Inst (None, 256, 256, 64) 128         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_41 (TFOpLambda)      (None, 256, 256, 64) 0           instance_normalization_56[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 128, 128, 128 73856       tf.nn.relu_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 128, 128, 128 147584      conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_57 (Inst (None, 128, 128, 128 256         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_42 (TFOpLambda)      (None, 128, 128, 128 0           instance_normalization_57[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 64, 64, 256)  295168      tf.nn.relu_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 64, 64, 256)  590080      conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_58 (Inst (None, 64, 64, 256)  512         conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_43 (TFOpLambda)      (None, 64, 64, 256)  0           instance_normalization_58[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 64, 64, 256)  590080      tf.nn.relu_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_59 (Inst (None, 64, 64, 256)  512         conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_44 (TFOpLambda)      (None, 64, 64, 256)  0           instance_normalization_59[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 64, 64, 256)  590080      tf.nn.relu_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_60 (Inst (None, 64, 64, 256)  512         conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 64, 64, 256)  0           instance_normalization_60[0][0]  \n",
            "                                                                 tf.nn.relu_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 64, 64, 256)  590080      add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_61 (Inst (None, 64, 64, 256)  512         conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_45 (TFOpLambda)      (None, 64, 64, 256)  0           instance_normalization_61[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 64, 64, 256)  590080      tf.nn.relu_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_62 (Inst (None, 64, 64, 256)  512         conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 64, 64, 256)  0           instance_normalization_62[0][0]  \n",
            "                                                                 add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 64, 64, 256)  590080      add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_63 (Inst (None, 64, 64, 256)  512         conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_46 (TFOpLambda)      (None, 64, 64, 256)  0           instance_normalization_63[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 64, 64, 256)  590080      tf.nn.relu_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_64 (Inst (None, 64, 64, 256)  512         conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 64, 64, 256)  0           instance_normalization_64[0][0]  \n",
            "                                                                 add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 64, 64, 256)  590080      add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_65 (Inst (None, 64, 64, 256)  512         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_47 (TFOpLambda)      (None, 64, 64, 256)  0           instance_normalization_65[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 64, 64, 256)  590080      tf.nn.relu_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_66 (Inst (None, 64, 64, 256)  512         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 64, 64, 256)  0           instance_normalization_66[0][0]  \n",
            "                                                                 add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 64, 64, 256)  590080      add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_67 (Inst (None, 64, 64, 256)  512         conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_48 (TFOpLambda)      (None, 64, 64, 256)  0           instance_normalization_67[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 64, 64, 256)  590080      tf.nn.relu_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_68 (Inst (None, 64, 64, 256)  512         conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 64, 64, 256)  0           instance_normalization_68[0][0]  \n",
            "                                                                 add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 64, 64, 256)  590080      add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_69 (Inst (None, 64, 64, 256)  512         conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_49 (TFOpLambda)      (None, 64, 64, 256)  0           instance_normalization_69[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 64, 64, 256)  590080      tf.nn.relu_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_70 (Inst (None, 64, 64, 256)  512         conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 64, 64, 256)  0           instance_normalization_70[0][0]  \n",
            "                                                                 add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 64, 64, 256)  590080      add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_71 (Inst (None, 64, 64, 256)  512         conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_50 (TFOpLambda)      (None, 64, 64, 256)  0           instance_normalization_71[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 64, 64, 256)  590080      tf.nn.relu_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_72 (Inst (None, 64, 64, 256)  512         conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 64, 64, 256)  0           instance_normalization_72[0][0]  \n",
            "                                                                 add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 64, 64, 256)  590080      add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_73 (Inst (None, 64, 64, 256)  512         conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_51 (TFOpLambda)      (None, 64, 64, 256)  0           instance_normalization_73[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 64, 64, 256)  590080      tf.nn.relu_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_74 (Inst (None, 64, 64, 256)  512         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 64, 64, 256)  0           instance_normalization_74[0][0]  \n",
            "                                                                 add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 128, 128, 128 295040      add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 128, 128, 128 147584      conv2d_transpose_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_75 (Inst (None, 128, 128, 128 256         conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_52 (TFOpLambda)      (None, 128, 128, 128 0           instance_normalization_75[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTrans (None, 256, 256, 64) 73792       tf.nn.relu_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_transpose_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_76 (Inst (None, 256, 256, 64) 128         conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_53 (TFOpLambda)      (None, 256, 256, 64) 0           instance_normalization_76[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 256, 256, 3)  9411        tf.nn.relu_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.tanh_1 (TFOpLambda)     (None, 256, 256, 3)  0           conv2d_96[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 11,129,667\n",
            "Trainable params: 11,129,667\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA6DDNnrdhex"
      },
      "source": [
        "\r\n",
        "#### Generator training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh5qzOVIAyPO"
      },
      "source": [
        "#TODO\r\n",
        "from glob import glob\r\n",
        "import os\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "import gc\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "def pretrain_step(vgg, input_images, generator, optimizer):\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        generated_images = generator(input_images)\r\n",
        "\r\n",
        "        #plt.imshow(generated_images[0])\r\n",
        "        #plt.imshow(input_images[0])\r\n",
        "        mae = tf.keras.losses.MeanAbsoluteError()\r\n",
        "        c_loss = mae(vgg(input_images), vgg(generated_images))\r\n",
        "\r\n",
        "        gradients = tape.gradient(c_loss, generator.trainable_variables)\r\n",
        "        optimizer.apply_gradients(zip(gradients, generator.trainable_variables))\r\n",
        "        content_loss_metric = tf.keras.metrics.Mean(\"content_loss\", dtype=tf.float32)\r\n",
        "        content_loss_metric(c_loss)\r\n",
        "\r\n",
        "def pretrain_generator(dataset_name,batch_size):\r\n",
        "    summary_writer = tf.summary.create_file_writer(os.path.join('/content/drive/My Drive/NN', \"pretrain\"))\r\n",
        "    print(f\"Starting to pretrain generator with 10 epochs...\")\r\n",
        "    print(f\"Building `{dataset_name}` dataset\")\r\n",
        "    dataset, steps_per_epoch = get_dataset(dataset_name=dataset_name, batch_size=batch_size)\r\n",
        "    \r\n",
        "    generator = G\r\n",
        "    #generator(tf.keras.Input(shape=(None,256,256, 3)))\r\n",
        "    #generator.summary()\r\n",
        "\r\n",
        "    print(\"Setting up optimizer to update generator's parameters...\")\r\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0008, beta_1=0.5)\r\n",
        "\r\n",
        "    trained_epochs = 0\r\n",
        "    epochs = 10\r\n",
        "\r\n",
        "    print(\"Proceeding pretraining without sample images...\")\r\n",
        "\r\n",
        "    print(\"Starting pre-training loop, setting up summary writer to record progress on TensorBoard...\")\r\n",
        "\r\n",
        "    for epoch in range(epochs):\r\n",
        "        epoch_idx = trained_epochs + epoch + 1\r\n",
        "\r\n",
        "        for step in tqdm(range(1, steps_per_epoch + 1), desc=f\"Pretrain Epoch {epoch + 1}/{epochs}\"):\r\n",
        "            image_batch = dataset.next()\r\n",
        "            pretrain_step(vgg, image_batch, generator, optimizer)\r\n",
        "\r\n",
        "            if step % 100 == 0: #100 = pretrain_reporting_steps\r\n",
        "                global_step = (epoch_idx - 1) * steps_per_epoch + step\r\n",
        "                with summary_writer.as_default():\r\n",
        "                    tf.summary.scalar('content_loss', tf.keras.metrics.Mean(\"content_loss\", dtype=tf.float32).result(), step=global_step)\r\n",
        "\r\n",
        "                tf.keras.metrics.Mean(\"content_loss\", dtype=tf.float32).reset_states()\r\n",
        "        gc.collect()\r\n",
        "    return generator\r\n",
        "        \r\n",
        "gen_pretrained = pretrain_generator(\"datasets/normal_photos/photos_from_COCO_resized\", 16)\r\n",
        "DATASET_NAME=\"pretrain\"\r\n",
        "savemodel(gen_pretrained,\"pretrain_generator_conv2d_batchNorm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUPngw40dkbA"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAhKHu3zQzrM"
      },
      "source": [
        "### Discriminitor batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StFKZT3Xdj6J"
      },
      "source": [
        "def create_discriminator(input_shape):\r\n",
        "    model = Sequential()\r\n",
        "    #k7n64s1\r\n",
        "    model.add(Conv2D(input_shape=input_shape, filters=32, kernel_size=3, strides=1, padding='same'))\r\n",
        "    model.add(LeakyReLU())\r\n",
        "\r\n",
        "    #k3n64s2\r\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, strides=2, padding='same'))\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "    \r\n",
        "    #k3n128s1\r\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, strides=1, padding='same'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "    \r\n",
        "    #k3n128s2\r\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, strides=2, padding='same'))\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "    #k3n256s1\r\n",
        "    model.add(Conv2D(filters=256, kernel_size=3, strides=1, padding='same'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "    #k3n256s1\r\n",
        "    model.add(Conv2D(filters=256, kernel_size=3, strides=1, padding='same'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "    #k3n1s1\r\n",
        "    model.add(Conv2D(filters=1, kernel_size=3, strides=1, padding='same'))\r\n",
        "   \r\n",
        "    \r\n",
        "    #model.compile(loss=tf.keras.losses.BinaryCrossentropy)\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "#D = create_discriminator(input_shape=(256,256,3))\r\n",
        "#D.summary()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSNNtpwaQ3wO"
      },
      "source": [
        "### Discriminator instance parametric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vnoi1jYdQ37v"
      },
      "source": [
        "def create_discriminator(input_shape):\r\n",
        "    model = Sequential()\r\n",
        "    #k7n64s1\r\n",
        "    model.add(Conv2D(input_shape=input_shape, filters=32, kernel_size=3, strides=1, padding='same'))\r\n",
        "    model.add(LeakyReLU())\r\n",
        "\r\n",
        "    #k3n64s2\r\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, strides=2, padding='same'))\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "    \r\n",
        "    #k3n128s1\r\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, strides=1, padding='same'))\r\n",
        "    model.add(tfa.layers.InstanceNormalization(axis=3, \r\n",
        "                                   center=True, \r\n",
        "                                   scale=True,\r\n",
        "                                   beta_initializer=\"random_uniform\",\r\n",
        "                                   gamma_initializer=\"random_uniform\",\r\n",
        "                                   epsilon=1e-05))\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "    \r\n",
        "    #k3n128s2\r\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, strides=2, padding='same'))\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "    #k3n256s1\r\n",
        "    model.add(Conv2D(filters=256, kernel_size=3, strides=1, padding='same'))\r\n",
        "    model.add(tfa.layers.InstanceNormalization(axis=3, \r\n",
        "                                   center=True, \r\n",
        "                                   scale=True,\r\n",
        "                                   beta_initializer=\"random_uniform\",\r\n",
        "                                   gamma_initializer=\"random_uniform\",\r\n",
        "                                   epsilon=1e-05))\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "    #k3n256s1\r\n",
        "    model.add(Conv2D(filters=256, kernel_size=3, strides=1, padding='same'))\r\n",
        "    model.add(tfa.layers.InstanceNormalization(axis=3, \r\n",
        "                                   center=True, \r\n",
        "                                   scale=True,\r\n",
        "                                   beta_initializer=\"random_uniform\",\r\n",
        "                                   gamma_initializer=\"random_uniform\",\r\n",
        "                                   epsilon=1e-05))\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "    #k3n1s1\r\n",
        "    model.add(Conv2D(filters=1, kernel_size=3, strides=1, padding='same'))\r\n",
        "   \r\n",
        "    \r\n",
        "    #model.compile(loss=tf.keras.losses.BinaryCrossentropy)\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "#D = create_discriminator(input_shape=(256,256,3))\r\n",
        "#D.summary()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5-wrb_TRKJO"
      },
      "source": [
        "### Discriminator instance without params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a65TqG3qRKTc"
      },
      "source": [
        "def create_discriminator(input_shape):\r\n",
        "    instance_param = tfa.layers.InstanceNormalization(axis=3, \r\n",
        "                                   center=True, \r\n",
        "                                   scale=True,\r\n",
        "                                   beta_initializer=\"random_uniform\",\r\n",
        "                                   gamma_initializer=\"random_uniform\")\r\n",
        "    model = Sequential()\r\n",
        "    #k7n64s1\r\n",
        "    model.add(Conv2D(input_shape=input_shape, filters=32, kernel_size=3, strides=1, padding='same'))\r\n",
        "    model.add(LeakyReLU())\r\n",
        "\r\n",
        "    #k3n64s2\r\n",
        "    model.add(Conv2D(filters=64, kernel_size=3, strides=2, padding='same'))\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "    \r\n",
        "    #k3n128s1\r\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, strides=1, padding='same'))\r\n",
        "    model.add(tfa.layers.InstanceNormalization())\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "    \r\n",
        "    #k3n128s2\r\n",
        "    model.add(Conv2D(filters=128, kernel_size=3, strides=2, padding='same'))\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "    #k3n256s1\r\n",
        "    model.add(Conv2D(filters=256, kernel_size=3, strides=1, padding='same'))\r\n",
        "    model.add(tfa.layers.InstanceNormalization())\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "    #k3n256s1\r\n",
        "    model.add(Conv2D(filters=256, kernel_size=3, strides=1, padding='same'))\r\n",
        "    model.add(tfa.layers.InstanceNormalization())\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "    #k3n1s1\r\n",
        "    model.add(Conv2D(filters=1, kernel_size=3, strides=1, padding='same'))\r\n",
        "   \r\n",
        "    \r\n",
        "    #model.compile(loss=tf.keras.losses.BinaryCrossentropy)\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "#D = create_discriminator(input_shape=(256,256,3))\r\n",
        "#D.summary()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwtFZGzY3ijx"
      },
      "source": [
        "## Adversarial model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5rUYAy0nCAj"
      },
      "source": [
        "### GAN preps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnvicjC5wMzi"
      },
      "source": [
        "#### Get datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L35BLKrqwQAW"
      },
      "source": [
        "from itertools import product\r\n",
        "from imageio import imwrite,imread\r\n",
        "import gc\r\n",
        "from IPython.display import Image,display\r\n",
        "\r\n",
        "\r\n",
        "def get_batches(dataset_name):\r\n",
        "    cartoon_set,steps_per_epoch = get_dataset(\"datasets/\"+dataset_name+\"/\"+dataset_name+\"_resized\",8)\r\n",
        "    cartoon_smoothed_set, steps_per_epoch = get_dataset(\"datasets/\"+dataset_name+\"/\"+dataset_name+SMOOTHED_N,8)\r\n",
        "    normal_photos_set,steps_per_epoch_normal = get_dataset(\"datasets/normal_photos/photos_from_COCO_resized\",8)\r\n",
        "\r\n",
        "    #New dataset name = photos_from_COCO_resized\r\n",
        "    dataset_kaggle, steps_per_epoch_coco = get_dataset(dataset_name=\"datasets/normal_photos/photos_from_kaggle_resized\", batch_size=16)\r\n",
        "    return cartoon_set, cartoon_smoothed_set, normal_photos_set, steps_per_epoch, dataset_kaggle\r\n",
        "\r\n",
        "\r\n",
        "def show_and_save_images(batch_x, image_name, nrow=2, ncol=4,to_be_saved=True):\r\n",
        "    if not isinstance(batch_x, np.ndarray):\r\n",
        "        batch_x = batch_x.numpy()\r\n",
        "    n, h, w, c = batch_x.shape\r\n",
        "    out_arr = np.zeros([h * nrow, w * ncol, 3], dtype=np.uint8)\r\n",
        "    for (i, j), k in zip(product(range(nrow), range(ncol)), range(n)):\r\n",
        "        out_arr[(h * i):(h * (i+1)), (w * j):(w * (j+1))] = batch_x[k]\r\n",
        "    path_name = os.path.join(\"/content/drive/My Drive/NN/training\", DATASET_NAME,TIME+\"_\"+OMEGA_STR+\"om_samples\")\r\n",
        "    check_folder(path_name)\r\n",
        "    path_name = os.path.join(path_name,image_name)\r\n",
        "    if to_be_saved:\r\n",
        "        imwrite(path_name, out_arr)\r\n",
        "    #display(Image(path_name,width=512, height=256))\r\n",
        "    gc.collect()\r\n",
        "    return out_arr\r\n",
        "\r\n",
        "def check_folder(log_dir):\r\n",
        "    if not os.path.exists(log_dir):\r\n",
        "        os.makedirs(log_dir)\r\n",
        "    return log_dir\r\n",
        "\r\n",
        "\r\n",
        "#show_and_save_images(tf.cast((gen_pretrained(seed, training=False) + 1) * 127.5, tf.uint8),\"test.png\" )"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn5dJVoCwDZj"
      },
      "source": [
        "#### Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANRjg4TiuXKA"
      },
      "source": [
        "mae = tf.keras.losses.MeanAbsoluteError()\r\n",
        "loss_bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n",
        "\r\n",
        "#Magia nera\r\n",
        "def gram(x):\r\n",
        "    shape_x = tf.shape(x)\r\n",
        "    b = shape_x[0]\r\n",
        "    c = shape_x[3]\r\n",
        "    x = tf.reshape(x, [b, -1, c])\r\n",
        "    return tf.matmul(tf.transpose(x, [0, 2, 1]), x) / tf.cast((tf.size(x) // b), tf.float32)\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def discriminator_loss (real, fake, real_blur):\r\n",
        "    real_loss = loss_bce(tf.ones_like(real), real)\r\n",
        "    fake_loss = loss_bce(tf.zeros_like(fake), fake)\r\n",
        "    real_blur_loss = loss_bce(tf.zeros_like(real_blur), real_blur)\r\n",
        "    #insert metrics\r\n",
        "    d_real_loss_metric(real_loss)\r\n",
        "    d_fake_loss_metric(fake_loss)\r\n",
        "    d_smooth_loss_metric(real_blur_loss)\r\n",
        "    return real_loss+fake_loss+real_blur_loss\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def generator_loss(fake):\r\n",
        "    return loss_bce(tf.ones_like(fake), fake)\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def style_loss(input_images, generated_images):\r\n",
        "    input_images = gram(input_images)\r\n",
        "    generated_images = gram(generated_images)\r\n",
        "    return mae(input_images, generated_images)\r\n",
        "\r\n",
        "\r\n",
        "g_total_loss_metric = tf.keras.metrics.Mean(\"g_total_loss\", dtype=tf.float32)\r\n",
        "g_adv_loss_metric = tf.keras.metrics.Mean(\"g_adversarial_loss\", dtype=tf.float32)\r\n",
        "content_loss_metric = tf.keras.metrics.Mean(\"content_loss\", dtype=tf.float32)\r\n",
        "style_loss_metric = tf.keras.metrics.Mean(\"style_loss\", dtype=tf.float32)\r\n",
        "d_total_loss_metric = tf.keras.metrics.Mean(\"d_total_loss\", dtype=tf.float32)\r\n",
        "d_real_loss_metric = tf.keras.metrics.Mean(\"d_real_loss\", dtype=tf.float32)\r\n",
        "d_fake_loss_metric = tf.keras.metrics.Mean(\"d_fake_loss\", dtype=tf.float32)\r\n",
        "d_smooth_loss_metric = tf.keras.metrics.Mean(\"d_smooth_loss\", dtype=tf.float32)\r\n",
        "style_loss_metric = tf.keras.metrics.Mean(\"style_loss\", dtype=tf.float32)\r\n",
        "\r\n",
        "training_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('training_accuracy', dtype=tf.float32)\r\n",
        "metric_and_names = [\r\n",
        "    (g_total_loss_metric, \"g_total_loss\"),\r\n",
        "    (g_adv_loss_metric, \"g_adversarial_loss\"),\r\n",
        "    (d_total_loss_metric, \"d_total_loss\"),\r\n",
        "    (d_real_loss_metric, \"d_real_loss\"),\r\n",
        "    (d_fake_loss_metric, \"d_fake_loss\"),\r\n",
        "    (d_smooth_loss_metric, \"d_smooth_loss\"),\r\n",
        "    (content_loss_metric, \"content_loss\"),\r\n",
        "    (style_loss_metric, \"style_loss\")\r\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1akWo_zM3G0W"
      },
      "source": [
        "#### Utils GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBoJBniM3BA9"
      },
      "source": [
        "def update_metrics(summary_writer,c_loss_tot,g_loss_tot,d_loss_tot,epoch,prec_epochs):\r\n",
        "    with summary_writer.as_default():\r\n",
        "        for metric, name in metric_and_names:\r\n",
        "            if name ==\"content_loss\":\r\n",
        "                tf.summary.scalar(\"generator/\"+name, np.mean(c_loss_tot), step=epoch+prec_epochs+1)\r\n",
        "            elif name == \"g_total_loss\":\r\n",
        "                tf.summary.scalar(\"generator/\"+name, np.mean(g_loss_tot), step=epoch+prec_epochs+1)\r\n",
        "            elif name == \"d_total_loss\":\r\n",
        "                tf.summary.scalar(\"discriminator/\"+name, np.mean(d_loss_tot), step=epoch+prec_epochs+1)\r\n",
        "            elif name.startswith(\"d\"):\r\n",
        "                tf.summary.scalar(\"discriminator/\"+name, metric.result(), step=epoch+prec_epochs+1)\r\n",
        "            else:\r\n",
        "                tf.summary.scalar(\"generator/\"+name, metric.result(), step=epoch+prec_epochs+1)\r\n",
        "            metric.reset_states()\r\n",
        "            \r\n",
        "def loads_models(model_name_gen=\"\",model_name_disc=\"\",checkpoint=False):\r\n",
        "    if EPOCHS_YET_TRAINED>0:\r\n",
        "        print(\"loading\")\r\n",
        "        gen_pretrained = loadmodel(model_name_gen,checkpoint)\r\n",
        "        D = loadmodel(model_name_disc,checkpoint)\r\n",
        "    else:\r\n",
        "        gen_pretrained = loadmodel(GENERATOR_NAME)\r\n",
        "        D = create_discriminator(input_shape=(256,256,3))\r\n",
        "    return gen_pretrained,D\r\n",
        "\r\n",
        "\r\n",
        "def best_epoch(g_list, d_list,g_loss_best,d_loss_best,epoch, prec_epochs):\r\n",
        "    #g_list = [(G1,g_loss1),(G2,g_loss2)]\r\n",
        "    #d_list = [(D1,d_loss1),(D2,d_loss2)]\r\n",
        "    from operator import itemgetter\r\n",
        "    g_new = min(g_list,key=itemgetter(1))\r\n",
        "    d_new = min(d_list,key=itemgetter(1))\r\n",
        "    if g_loss_best>g_new[1]:\r\n",
        "        g_loss_best = g_new[1]\r\n",
        "        savemodel(g_new[0],MODEL_NAME_GEN+\"_BEST_\"+str(epoch+prec_epochs)+\"ep\",checkpoint=True)\r\n",
        "    if d_loss_best>d_new[1]:\r\n",
        "        d_loss_best = d_new[1]\r\n",
        "        savemodel(d_new[0],MODEL_NAME_DISC+\"_BEST_\"+str(epoch+prec_epochs)+\"ep\",checkpoint=True)\r\n",
        "    return g_loss_best, d_loss_best\r\n",
        "                  \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4y0r_NF-Uw1"
      },
      "source": [
        "#### Training GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frlQ1ENy6Ufu"
      },
      "source": [
        "import time\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "import datetime, os\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def train_step(cartoon_set_batch, cartoon_smoothed_batch, normal_photos_batch, generator, discriminator):\r\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\r\n",
        "        generated_images = generator(normal_photos_batch, training=True)\r\n",
        "\r\n",
        "        real_output = discriminator(cartoon_set_batch, training=True)\r\n",
        "        fake_output = discriminator(generated_images, training=True)\r\n",
        "        real_smooth_output = discriminator(cartoon_smoothed_batch, training=True)\r\n",
        "        \r\n",
        "        disc_loss = discriminator_loss(real_output, fake_output, real_smooth_output)\r\n",
        "        \r\n",
        "        gen_adv_loss = generator_loss(fake_output) * G_ADV_LAMBDA\r\n",
        "        c_loss = mae(vgg(normal_photos_batch), vgg(generated_images))\r\n",
        "        gen_total_loss = gen_adv_loss + c_loss*OMEGA \r\n",
        "\r\n",
        "        if L1 > -1:\r\n",
        "            s_loss = L1 * style_loss(vgg(cartoon_set_batch[:vgg(generated_images).shape[0]]),vgg(generated_images))\r\n",
        "            gen_total_loss = gen_total_loss + s_loss\r\n",
        "            \r\n",
        "        \r\n",
        "    gradients_of_generator = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\r\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\r\n",
        "\r\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\r\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\r\n",
        "\r\n",
        "    g_total_loss_metric(gen_total_loss)\r\n",
        "    g_adv_loss_metric(gen_adv_loss)\r\n",
        "    content_loss_metric(c_loss)\r\n",
        "    d_total_loss_metric(disc_loss)\r\n",
        "    if L1 > -1:\r\n",
        "        style_loss_metric(s_loss)\r\n",
        "    \r\n",
        "    return disc_loss, gen_total_loss, c_loss \r\n",
        "\r\n",
        "def train(dataset_name,cartoon_set, cartoon_smoothed_set, normal_photos_set, epochs,prec_epochs, gen_pretrained, D):\r\n",
        "    summary_writer = tf.summary.create_file_writer(\"/content/drive/My Drive/NN/training/\"+dataset_name+\"/\"+TIME+\"_\"+OMEGA_STR+\"om_logs/\")\r\n",
        "    with summary_writer.as_default():\r\n",
        "        img = np.expand_dims(show_and_save_images(tf.cast((normal_photos_set.next() + 1) * 127.5, tf.uint8),image_name=\"normal_sample_images.png\"), 0,)\r\n",
        "        tf.summary.image(\"normal_images_\"+dataset_name, img, step=0)\r\n",
        "        img = np.expand_dims(show_and_save_images(tf.cast((cartoon_set.next() + 1) * 127.5, tf.uint8),\r\n",
        "            image_name=\"cartoon_sample_images_\"+dataset_name+\".png\"), 0,)\r\n",
        "        tf.summary.image(\"cartoon_sample_images_\"+dataset_name, img, step=0)\r\n",
        "\r\n",
        "    g_list, d_list = [], []\r\n",
        "    g_loss_best,d_loss_best = 99, 99\r\n",
        "    for epoch in tqdm(range(0,epochs), total = epochs):\r\n",
        "        start = time.time()\r\n",
        "        d_loss_tot,g_loss_tot,c_loss_tot = [],[],[]\r\n",
        "        for step in tqdm(range(1, steps_per_epoch + 1), desc=f'Train {epoch + 1+prec_epochs}/{epochs+prec_epochs}', total=steps_per_epoch):\r\n",
        "            cart_batch = cartoon_set.next()\r\n",
        "            cart_smooth_batch = cartoon_smoothed_set.next()\r\n",
        "            norm_photos_batch = normal_photos_set.next()\r\n",
        "\r\n",
        "            d_loss,g_loss,c_loss = train_step(cart_batch, cart_smooth_batch, norm_photos_batch,gen_pretrained,D)\r\n",
        "            d_loss_tot.append(d_loss)\r\n",
        "            g_loss_tot.append(g_loss)\r\n",
        "            c_loss_tot.append(c_loss)\r\n",
        "        \r\n",
        "        update_metrics(summary_writer,c_loss_tot,g_loss_tot,d_loss_tot,epoch,prec_epochs)\r\n",
        "        g_list.append((gen_pretrained,np.mean(g_loss_tot)))\r\n",
        "        d_list.append((D,np.mean(d_loss_tot)))\r\n",
        "\r\n",
        "        #print(\"Generated images at epoch \"f\"{epoch+1+prec_epochs}\")\r\n",
        "        fake_batch = tf.cast( (gen_pretrained(SEED, training=False) + 1) * 127.5, tf.uint8)\r\n",
        "        img = np.expand_dims(show_and_save_images(\r\n",
        "                fake_batch,\r\n",
        "                image_name=(\"gan_generated_images_\"+dataset_name+\"_at_epoch_\"f\"{epoch+1+prec_epochs}_\"+TIME+\".png\")\r\n",
        "                ),0,\r\n",
        "            )\r\n",
        "        with summary_writer.as_default():\r\n",
        "            tf.summary.image('gan_generated_images_'+dataset_name, img, step=epoch+1+prec_epochs)\r\n",
        "        #display  normal\r\n",
        "        \r\n",
        "        #print(\"Normal images at epoch \"f\"{epoch+1+prec_epochs}\")\r\n",
        "        if epoch==0: \r\n",
        "            show_and_save_images(tf.cast((SEED+1) * 127.5, tf.uint8), image_name=(\"normal_images_\"+dataset_name+\"_\"+TIME+\".png\"),to_be_saved=True)\r\n",
        "        else:\r\n",
        "            show_and_save_images(tf.cast((SEED+1) * 127.5, tf.uint8), image_name=(\"normal_images_\"+dataset_name+\"_\"+TIME+\".png\"),to_be_saved=False)\r\n",
        "            if (epoch+1+prec_epochs)% CHECK_AFTER_X_EPOCH == 0:\r\n",
        "                savemodel(D,MODEL_NAME_DISC+\"_checkpoint_\"+str(epoch+1+prec_epochs)+\"ep\",checkpoint=True)\r\n",
        "                savemodel(gen_pretrained,MODEL_NAME_GEN+\"_checkpoint_\"+str(epoch+1+prec_epochs)+\"ep\",checkpoint=True)\r\n",
        "\r\n",
        "                g_loss_best, d_loss_best = best_epoch(g_list, d_list,g_loss_best,d_loss_best,epoch,prec_epochs)\r\n",
        "                g_list, d_list = [], []\r\n",
        "\r\n",
        "        print(\"Epoch:  [%5d/%5d] time: %4.4f content_loss: %.8f\" % (epoch+1+prec_epochs, epochs+prec_epochs, time.time() - start, np.mean(c_loss_tot)))\r\n",
        "        print(\"Epoch:  [%5d/%5d] time: %4.4f disc_loss: %.8f, gen_loss: %.8f\" % (epoch+1+prec_epochs, epochs+prec_epochs, time.time() - start, np.mean(d_loss_tot), np.mean(g_loss_tot)))\r\n",
        "\r\n",
        "    # Generate after the final epoch\r\n",
        "    return gen_pretrained, D\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdniBrgwx7hM"
      },
      "source": [
        "### Setup and Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwdWEJ_4x8CJ"
      },
      "source": [
        "%load_ext tensorboard\r\n",
        "\r\n",
        "## SETUP ##\r\n",
        "DATASET_NAME = \"paprika\"\r\n",
        "EPOCHS = 15\r\n",
        "EPOCHS_YET_TRAINED = 0\r\n",
        "OMEGA_STR = \"0_4\"\r\n",
        "SMOOTHED_N = \"_resized_smoothed_2\"\r\n",
        "\r\n",
        "OMEGA = 0.4 # paper value default = 10\r\n",
        "L1 = 25 # paper value default = 1\r\n",
        "G_ADV_LAMBDA = 8 #paper value default = 1\r\n",
        "GEN_LR = 8e-5 #paper value default = 1e-5\r\n",
        "DISC_LR = 3e-5 #paper value default = 1e-5\r\n",
        "generator_optimizer = keras.optimizers.Adam(learning_rate=GEN_LR, beta_1=.5) \r\n",
        "discriminator_optimizer = keras.optimizers.Adam(learning_rate=DISC_LR, beta_1=.5) \r\n",
        "\r\n",
        "\r\n",
        "CHECKPOINT = True\r\n",
        "CHECK_AFTER_X_EPOCH = 5\r\n",
        "## change name of checkpoint\r\n",
        "MODEL_NAME_DISC_CP = \"convTrans2d_instanceNorm_paprika_discriminator_85ep_19-02_12-08_om=0_4_l=25_checkpoint_70ep\"\r\n",
        "MODEL_NAME_GEN_CP = \"convTrans2d_instanceNorm_paprika_generator_85ep_19-02_12-08_om=0_4_l=25_checkpoint_70ep\"\r\n",
        "GENERATOR_NAME =\"pretrain_generator_convTrans2d_instanceNorm\"\r\n",
        "\r\n",
        "TIME = time.strftime(\"%d-%m_%H-%M\", time.gmtime())\r\n",
        "\r\n",
        "if CHECKPOINT:\r\n",
        "    TIME = MODEL_NAME_DISC_CP[MODEL_NAME_DISC_CP.find(\"p_\")+2:MODEL_NAME_DISC_CP.find(\"_o\")]\r\n",
        "    GENERATOR_NAME = \"pretrain_generator_\"+MODEL_NAME_DISC_CP[:MODEL_NAME_DISC_CP.find(DATASET_NAME)-1]\r\n",
        "    if \"check\" in MODEL_NAME_DISC_CP:\r\n",
        "        gen_pretrained, D = loads_models(MODEL_NAME_GEN_CP,MODEL_NAME_DISC_CP,CHECKPOINT)\r\n",
        "    else:\r\n",
        "        gen_pretrained, D = loads_models(MODEL_NAME_GEN_CP,MODEL_NAME_DISC_CP)\r\n",
        "else:\r\n",
        "    gen_pretrained, D = loads_models()\r\n",
        "\r\n",
        "MODEL_NAME_DISC =  GENERATOR_NAME[GENERATOR_NAME.find(\"r_\")+2:] +\"_\" +DATASET_NAME+'_discriminator_'+str(EPOCHS_YET_TRAINED+EPOCHS)+\"ep_\"+TIME +\"_om=\"+ OMEGA_STR + \"_l=\" + str(L1)\r\n",
        "MODEL_NAME_GEN =  GENERATOR_NAME[GENERATOR_NAME.find(\"r_\")+2:] + \"_\"+DATASET_NAME+'_generator_'+str(EPOCHS_YET_TRAINED+EPOCHS)+\"ep_\"+TIME + \"_om=\"+ OMEGA_STR + \"_l=\" + str(L1)\r\n",
        "## Load datasets ##\r\n",
        "cartoon_set, cartoon_smoothed_set, normal_photos_set, steps_per_epoch , dataset_kaggle = get_batches(DATASET_NAME)\r\n",
        "SEED = dataset_kaggle.next()\r\n",
        "\r\n",
        "## Training ##\r\n",
        "%tensorboard --logdir_spec={DATASET_NAME}:\"/content/drive/My Drive/NN/training/{DATASET_NAME}/{TIME}_{OMEGA_STR}om_logs\" \r\n",
        "generator_trained, discriminator_trained = train(DATASET_NAME,cartoon_set, cartoon_smoothed_set, normal_photos_set, EPOCHS,EPOCHS_YET_TRAINED, gen_pretrained, D)\r\n",
        "\r\n",
        "## Saving final models ##\r\n",
        "savemodel(discriminator_trained,MODEL_NAME_DISC)\r\n",
        "savemodel(generator_trained,MODEL_NAME_GEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDaL8jlfdQi8"
      },
      "source": [
        "### Create gif"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vs1VgHHjq41b"
      },
      "source": [
        "#generator_trained.save_weights(os.path.join(models_dir, \"generator_your_name_15\"))\r\n",
        "#%tensorboard --logdir \"/content/drive/My Drive/NN/training/logs_your_name\"\r\n",
        "!pip install git+https://github.com/tensorflow/docs\r\n",
        "import imageio\r\n",
        "import tensorflow_docs.vis.embed as embed\r\n",
        "from glob import glob\r\n",
        "def create_gif(dataset_name):\r\n",
        "    anim_file = 'dcgan'+dataset_name+'.gif'\r\n",
        "    with imageio.get_writer(anim_file, mode='I') as writer:\r\n",
        "        filenames = glob('/content/drive/My Drive/NN/training/'+dataset_name+'_samples/gan_generated_images_'+dataset_name+'_at_epoch_*_'+TIME+'.png')\r\n",
        "        filenames = sorted(filenames)\r\n",
        "        for filename in filenames:\r\n",
        "            image = imageio.imread(filename)\r\n",
        "            writer.append_data(image)\r\n",
        "        #image = imageio.imread(filename)\r\n",
        "        #writer.append_data(image)\r\n",
        "    return anim_file\r\n",
        "\r\n",
        "#anim_file = create_gif(dataset_name)\r\n",
        "#embed.embed_file(anim_file)\r\n",
        "gen_pretrained.summary()     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upNrByYgjzAd"
      },
      "source": [
        "# 4 - Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUPb7Ltzg5wW"
      },
      "source": [
        "%load_ext tensorboard\r\n",
        "#%tensorboard --logdir_spec=spirited_away:\"/content/drive/My Drive/NN/training/logs_spirited_away_09-02_09-58\",your_name:\"/content/drive/My Drive/NN/training/logs_your_name\" #DATASET_NAME +_TIME\r\n",
        "dataset, steps_per_epoch = get_dataset(dataset_name='datasets/test_img/test_img_resized', batch_size=8)\r\n",
        "seed = dataset.next()\r\n",
        "TIME = \"test\"\r\n",
        "OMEGA_STR = \"tuasorella\"\r\n",
        "DATASETS=[\"paprika\",\"your_name\",\"spirited_away\"]\r\n",
        "GENERATORS =[\"convTrans2d_instanceNorm_spirited_away_generator_80ep_23-02_11-13_om=0_l=1_checkpoint_65ep\",\r\n",
        "             \"spirited_away_generator_150ep_12-02_10-00_om=10\",\r\n",
        "             \"paprika_generator_130ep_10-02_09-42\",\r\n",
        "             \"convTrans2d_instanceNorm_paprika_generator_75ep_21-02_17-47_om=0_000005_l=-2_checkpoint_65ep\",\r\n",
        "             \"convTrans2d_instanceNorm_paprika_generator_50ep_19-02_11-14_om=0_004_checkpoint_45ep\",\r\n",
        "             \"convTrans2d_instanceNorm_spirited_away_discriminator_80ep_22-02_01-14_om=0_04_l=5_checkpoint_75ep\",\r\n",
        "             \"convTrans2d_instanceNorm_spirited_away_generator_40ep_22-02_23-57_om=0_1_l=2_checkpoint_20ep\",\r\n",
        "             \"padding_conv2d_instanceNorm_paprika_generator_100ep_18-02_15-53_om=0_4_checkpoint_90ep\",\r\n",
        "             \"convTrans2d_instanceNorm_paprika_generator_120ep_19-02_12-08_om=0_4_l=25\",\r\n",
        "             \"convTrans2d_instanceNorm_spirited_away_generator_40ep_21-02_15-53_om=0_4_l=25_checkpoint_20ep\",\r\n",
        "             \"convTrans2d_instanceNorm_spirited_away_generator_40ep_21-02_14-01_om=0_4_l=25_checkpoint_35ep\",\r\n",
        "             \"convTrans2d_instanceNorm_paprika_generator_80ep_20-02_23-48_om=0_4_l=25_checkpoint_35ep\",\r\n",
        "             \"convTrans2d_batchNorm_spirited_away_generator_70ep_15-02_13-41_om=0_000005_checkpoint_35ep\",\r\n",
        "             \"convTrans2d_batchNorm_spirited_away_generator_40ep_16-02_15-57_om=0_000005_checkpoint_35ep\",\r\n",
        "             \"convTrans2d_batchNorm_paprika_generator_50ep_19-02_10-37_om=0_4\",\r\n",
        "             \"convTrans2d_batchNorm_paprika_generator_55ep_22-02_21-40_om=0_4_l=1\",\r\n",
        "             \"padding_conv2d_instanceNorm_paprika_generator_90ep_18-02_15-11_om=10\",\r\n",
        "             \"padding_conv2d_instance_spirited_away_generator_20ep_15-02_14-34_om=0_000005_checkpoint_20ep\",\r\n",
        "             \"padding_conv2d_instanceNorm_spirited_away_generator_40ep_17-02_17-16_om=0_000005_checkpoint_30ep\",\r\n",
        "             \"padding_conv2d_instanceNorm_spirited_away_generator_60ep_17-02_18-23_om=1_checkpoint_40ep\",\r\n",
        "             \"padding_conv2d_instance_spirited_away_generator_150ep_15-02_22-12_om=10_checkpoint_150ep\",\r\n",
        "             \"padding_conv2d_instance_spirited_away_generator_150ep_15-02_22-12_om=10\",\r\n",
        "             \"convtrans2d_instanceNormWithParamsNEW_paprika_generator_200ep_28-02_14-20_om=0_000005_l=5_checkpoint_200ep\",\r\n",
        "             \"convtrans2d_instanceNormWithParamsNEW_paprika_generator_170ep_01-03_09-47_om=0_000005_l=5\",\r\n",
        "             #convtrans2d_instanceNormWithParamsNEW_spirited_away_generator_60ep_24-02_15-13_om=0_4_l=25_checkpoint_30ep\r\n",
        "             \"convtrans2d_instanceNormWithParamsNEW_paprika_generator_200ep_28-02_11-35_om=0_4_l=25_checkpoint_170ep\",\r\n",
        "             \"convtrans2d_instanceNormWithParamsNEW_paprika_generator_150ep_28-02_18-44_om=0_4_l=25_checkpoint_140ep\"\r\n",
        "             \r\n",
        "             ]\r\n",
        "GENERATORS_NOT_TRAINED = [\"pretrain_generator_convTrans2d_batchNorm\",\r\n",
        "             \"pretrain_generator_padding_conv2d_batchNorm\",\r\n",
        "             \"pretrain_generator_convTrans2d_instanceNorm\",\r\n",
        "             \"pretrain_generator_convtrans2d_instanceNormWithParams\",\r\n",
        "             \"pretrain_generator_padding_conv2d_instanceNorm\",\r\n",
        "             \"pretrain_generator_conv2d_batchNorm\",\r\n",
        "             \"pretrain_generator_convtrans2d_instanceNormWithParamsNEW\"\r\n",
        "             ]\r\n",
        "def find_d(name):\r\n",
        "    check = False\r\n",
        "    if \"check\" in name:\r\n",
        "        check = True\r\n",
        "    for d in DATASETS: \r\n",
        "        if d in name:\r\n",
        "            return d, check\r\n",
        "    return \"pretrain\", check\r\n",
        "    \r\n",
        "def collect_results(generators):\r\n",
        "    for name in generators:\r\n",
        "        global DATASET_NAME\r\n",
        "        DATASET_NAME,check = find_d(name)\r\n",
        "        gen_pretrained = loadmodel(name,check)\r\n",
        "        DATASET_NAME = \"pretrain\"\r\n",
        "        show_and_save_images(tf.cast((gen_pretrained(seed)+1) * 127.5, tf.uint8), image_name=(name+\".png\"), )\r\n",
        "    show_and_save_images(tf.cast(((seed)+1) * 127.5, tf.uint8), image_name=(\"normal.png\"), )\r\n",
        "\r\n",
        "\r\n",
        "def show_graph(gen_name):\r\n",
        "    _time = gen_name[gen_name.find(\"p_\")+2:gen_name.find(\"_o\")]\r\n",
        "    if not \"l=\" in gen_name:\r\n",
        "        om = gen_name[gen_name.find(\"om=\")+3:gen_name.find(\"_che\")]\r\n",
        "    else:\r\n",
        "        om = gen_name[gen_name.find(\"om=\")+3:gen_name.find(\"l\")-1]\r\n",
        "    dataset,check = find_d(gen_name)\r\n",
        "    path = \"/content/drive/My Drive/NN/training/\" + dataset + \"/\" + _time +\"_\"+om+\"om_logs\" #\"22-02_01-14_0_04om_logs\"\r\n",
        "    print(path)\r\n",
        "    %tensorboard --logdir_spec={gen_name}:\"{path}\"\r\n",
        "\r\n",
        "DATASET_NAME = \"pretrain\"\r\n",
        "#show_graph(\"convtrans2d_instanceNormWithParamsNEW_spirited_away_generator_60ep_24-02_15-13_om=0_4_l=25_checkpoint_30ep\")\r\n",
        "collect_results(GENERATORS_NOT_TRAINED)\r\n",
        "collect_results(GENERATORS)\r\n",
        "\r\n",
        "#spirited_away_04_p:\"/content/drive/My Drive/NN/training/spirited_away/21-02_14-01_0_4om_logs\"#,spirited_away_10:\"/content/drive/My Drive/NN/training/spirited_away/15-02_22-12_10om_logs\",spirited_away_batch_005:\"/content/drive/My Drive/NN/training/spirited_away/15-02_14-34_0_000005om_logs\",spirited_away_transp:\"/content/drive/My Drive/NN/training/spirited_away/12-02_10-00_10om_logs\",spirited_away_transp_batch_nopad:\"/content/drive/My Drive/NN/training/spirited_away/15-02_13-41_0_000005om_logs\",spirited_away_transp_batch_nopad_dd:\"/content/drive/My Drive/NN/training/spirited_away/16-02_15-57_0_000005om_logs\"\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY2-U4p_kX3_"
      },
      "source": [
        "#%load_ext tensorboard\r\n",
        "#%tensorboard --logdir_spec=spirited_away_1:\"/content/drive/My Drive/NN/training/spirited_away/17-02_18-23_1om_logs\",spirited_away_0005:\"/content/drive/My Drive/NN/training/spirited_away/17-02_17-16_0_000005om_logs\",spirited_away_10:\"/content/drive/My Drive/NN/training/spirited_away/15-02_22-12_10om_logs\",spirited_away_batch_005:\"/content/drive/My Drive/NN/training/spirited_away/15-02_14-34_0_000005om_logs\",spirited_away_transp:\"/content/drive/My Drive/NN/training/spirited_away/12-02_10-00_10om_logs\",spirited_away_transp_batch_nopad:\"/content/drive/My Drive/NN/training/spirited_away/15-02_13-41_0_000005om_logs\",spirited_away_transp_batch_nopad_dd:\"/content/drive/My Drive/NN/training/spirited_away/16-02_15-57_0_000005om_logs\"\r\n",
        "#%tensorboard --logdir_spec=spirited_away_1:\"/content/drive/My Drive/NN/training/paprika/10-02_09-42_logs\"\r\n",
        "#DATASET_NAME =\"paprika\"\r\n",
        "g = loadmodel(\"pretrain_generator_convTrans2d_instanceNorm\",checkpoint=False)\r\n",
        "g_2 = loadmodel(\"pretrain_generator_padding_conv2d_instanceNorm\",checkpoint=False)\r\n",
        "g.summary()\r\n",
        "g_2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}